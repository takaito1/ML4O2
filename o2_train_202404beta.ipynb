{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e6a8d6-ed00-46a5-8a64-11780ff021fe",
   "metadata": {},
   "source": [
    "# Training algorithm for O2 gapfill\n",
    "    - Needs to enter a 6 digit input parameter as follows : \n",
    "    - First digit = Algorithm type (1=RF, 2=NN)\n",
    "    - Second digit = Data Source (1=Ship only, 2=Ship+Argo)\n",
    "    - Third digit = Ocean basin (1=Atlantic, 2=Pacific, 3=Indian, 4=Southern, 5=Arctic)\n",
    "    - Fourth digit = T/S data source (1=EN4)\n",
    "    - Fifth digit = predictor variable set (1=default, 2=cos/sin_month)\n",
    "    - Sixth digit = hyperparameter set (1=default, 2=preset hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f4841c-2e34-4013-bbc0-a776d4f3d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import sklearn as skl\n",
    "import gsw\n",
    "import cartopy.crs as ccrs\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efe3e10-117d-4dab-8aac-b95b56eec428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version information\n",
    "ver = '2.1.1.1.2.4'\n",
    "date1='04132024' # Set this for saving today's date. Usually date1=today's date\n",
    "date2='04132024' # Set alternative date for re-running previous results\n",
    "rerun = False    # indicate again whether you are re-running previous results\n",
    "#\n",
    "! mkdir -p /glade/derecho/scratch/ito/ML4O2_temp\n",
    "dirout='/glade/derecho/scratch/ito/ML4O2_temp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9f73c-c4ce-4295-a1c4-4cab970a2e43",
   "metadata": {},
   "source": [
    "### display selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ded563-9d73-41cd-9359-fd5b553e5630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network algorithm will be used.\n",
      "Ship-based O2 data will be used. Year_end = 2011\n",
      "Atlantic Ocean will be mapped\n",
      "EN4 dataset will be used for T/S input. \n",
      "Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month)\n",
      "New K-fold cross validation\n"
     ]
    }
   ],
   "source": [
    "selection = ver.split('.')\n",
    "basin = ['Atlantic','Pacific','Indian','Southern','Arctic']\n",
    "#\n",
    "if selection[0] == '1':\n",
    "    print('Random Forst algorithm will be used.')\n",
    "    alg = 'RF'\n",
    "elif selection[0] == '2':\n",
    "    print('Neural Network algorithm will be used.')\n",
    "    alg = 'NN'\n",
    "else:\n",
    "    print('error - incorrect algorithm type')\n",
    "#\n",
    "if selection[1] == '1':\n",
    "    print('Ship-based O2 data will be used. Year_end = 2011')\n",
    "    endyear=2011\n",
    "elif selection[1] == '2':\n",
    "    print('Ship-based and Argo-O2 data will be used. Year_end = 2021')\n",
    "    endyear=2021\n",
    "else:\n",
    "    print('error - incorrect input data type')\n",
    "#\n",
    "if selection[2] == '1':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "elif selection[2] == '2':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "elif selection[2] == '3':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "elif selection[2] == '4':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "elif selection[2] == '5':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "else:\n",
    "    print('error - incorrect O2 data type')\n",
    "#\n",
    "if selection[3] == '1':\n",
    "    print('EN4 dataset will be used for T/S input. ')\n",
    "else:\n",
    "    print('error - incorrect T/S data type')\n",
    "#\n",
    "if selection[4] == '1':\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, month')\n",
    "elif selection[4] == '2':\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month)')\n",
    "elif selection[4] == '3':\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month), sigma')\n",
    "elif selection[4] == '4':\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month), sigma, N2')\n",
    "else:\n",
    "    print('error - incorrect predictor variable type')\n",
    "#\n",
    "if selection[5] == '1':\n",
    "    print('Hyperparameter set is optimized via K-fold CV')\n",
    "elif selection[5] == '2':\n",
    "    print('A pre-set hyperparameter set is used')\n",
    "elif selection[5] == '4':\n",
    "    print('New K-fold cross validation')\n",
    "else:\n",
    "    print('error - incorrect hyperparameter type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423dfb32-5f0d-4dac-a81e-b79f298992c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input and output folders\n",
    "#\n",
    "os.system('echo $USER > userid')\n",
    "usrid=np.genfromtxt('userid',dtype='<U8')\n",
    "os.system('rm userid')\n",
    "diro = '/glade/derecho/scratch/'+str(usrid)+'/WOD18_OSDCTD/'\n",
    "dirf = '/glade/campaign/univ/ugit0034/EN4/L09_20x180x360/'\n",
    "dirin = '/glade/campaign/univ/ugit0034/WOD18_OSDCTD/'\n",
    "fargo = '/glade/campaign/univ/ugit0034/bgcargo/o2_Global_ARGO_Type12_47lev.nc'\n",
    "fosd='_1x1bin_osd_'\n",
    "fctd='_1x1bin_ctd_'\n",
    "fmer='_1x1bin_osdctd_'\n",
    "var=['o2','TSN2']\n",
    "os.system('mkdir -p '+diro)\n",
    "os.system('mkdir -p '+diro+'temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef87a60-be95-45c2-aa93-ae160eba2e65",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2024504-3ce0-42a4-873a-ed865e925333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain vertical grid\n",
    "ds=xr.open_dataset(dirin+var[0]+fmer+str(1965)+'.nc')\n",
    "Z=ds.depth.to_numpy()\n",
    "Nz=np.size(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f620564-b94a-41dc-bfb1-f9b9360fca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select analysis period\n",
    "# do not change the start year from 1965 (this is when Carpenter 1965 established modern Winkler method)\n",
    "yrs=np.arange(1965,endyear,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32422c9d-d461-464b-b88a-9f1c12c5e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlantic Ocean will be mapped\n"
     ]
    }
   ],
   "source": [
    "# basin-specific input data loading\n",
    "dsm=xr.open_dataset('/glade/campaign/univ/ugit0034/wod18/basin_mask_01.nc')\n",
    "#\n",
    "if selection[2] == '1':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "    bname0='atlantic'\n",
    "elif selection[2] == '2':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "    bname0='pacific'\n",
    "elif selection[2] == '3':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "    bname0='indian'\n",
    "elif selection[2] == '4':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "    bname0='southern'\n",
    "elif selection[2] == '5':\n",
    "    print(basin[int(selection[2])-1]+' Ocean will be mapped')\n",
    "    bname0='arctic'\n",
    "else:\n",
    "    print('error - incorrect O2 data type')\n",
    "#\n",
    "if selection[1]=='2':\n",
    "    doa1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/o20_{bname0}_1x1_47lev.npy')\n",
    "    dta1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/t0_{bname0}_1x1_47lev.npy')\n",
    "    dsa1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/s0_{bname0}_1x1_47lev.npy')\n",
    "    xx1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/lon0_{bname0}_1x1_47lev.npy')\n",
    "    yy1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/lat0_{bname0}_1x1_47lev.npy')\n",
    "    zz1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/depth0_{bname0}_1x1_47lev.npy')\n",
    "    tt1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/time0_{bname0}_1x1_47lev.npy')\n",
    "    tc1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/month0_{bname0}_1x1_47lev.npy')\n",
    "    dsga1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/sigma0_{bname0}_1x1_47lev.npy')\n",
    "    dn2a1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/N20_{bname0}_1x1_47lev.npy')\n",
    "elif selection[1]=='1':\n",
    "    doa1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/o20_{bname0}_1x1_47lev_ship.npy')\n",
    "    dta1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/t0_{bname0}_1x1_47lev_ship.npy')\n",
    "    dsa1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/s0_{bname0}_1x1_47lev_ship.npy')\n",
    "    xx1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/lon0_{bname0}_1x1_47lev_ship.npy')\n",
    "    yy1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/lat0_{bname0}_1x1_47lev_ship.npy')\n",
    "    zz1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/depth0_{bname0}_1x1_47lev_ship.npy')\n",
    "    tt1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/time0_{bname0}_1x1_47lev_ship.npy')\n",
    "    tc1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/month0_{bname0}_1x1_47lev_ship.npy')\n",
    "    dsga1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/sigma0_{bname0}_1x1_47lev_ship.npy')\n",
    "    dn2a1 = np.load(f'/glade/campaign/univ/ugit0034/ML4O2/input_202404/N20_{bname0}_1x1_47lev_ship.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b90748a-7809-4d62-b5c5-3c809cac4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120442\n"
     ]
    }
   ],
   "source": [
    "Nsample = np.size(doa1)\n",
    "print(Nsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687e2e3-71f2-4faf-bd44-c3b573406f2b",
   "metadata": {},
   "source": [
    "### This is where we choose what variables to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b30889-4104-489c-8dbe-e6230ec17598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month)\n"
     ]
    }
   ],
   "source": [
    "# generate data matrix and standardize it\n",
    "if selection[4] == '1':\n",
    "    X = np.array([dsa1, dta1, xx1, yy1, zz1, tt1, tc1])\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, month')\n",
    "elif selection[4] == '2':\n",
    "    X = np.array([dsa1, dta1, xx1, yy1, zz1, tt1, np.cos(2*np.pi*tc1/12), np.sin(2*np.pi*tc1/12)])\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month)')\n",
    "elif selection[4] == '3':\n",
    "    X = np.array([dsa1, dta1, xx1, yy1, zz1, tt1, np.cos(2*np.pi*tc1/12), np.sin(2*np.pi*tc1/12), dsga1])\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month), sigma')\n",
    "elif selection[4] == '4':\n",
    "    X = np.array([dsa1, dta1, xx1, yy1, zz1, tt1, np.cos(2*np.pi*tc1/12), np.sin(2*np.pi*tc1/12), dsga1, dn2a1])\n",
    "    print('Predictor variables include T, S, lon, lat, depth (pressure), year, cos(month), sin(month), sigma, N2')\n",
    "else:\n",
    "    print('error - incorrect predictor variable type')    \n",
    "#X = np.array([dsa1, dta1, xx1, yy1, tt1, tc1])\n",
    "#\n",
    "y = doa1\n",
    "#\n",
    "Xm = np.mean(X,axis=1)\n",
    "Xstd = np.std(X,axis=1)\n",
    "#\n",
    "N=np.size(y)\n",
    "# normalize x and y\n",
    "Xa = (X.T - Xm)/Xstd\n",
    "ym = np.mean(y)\n",
    "ystd = np.std(y)\n",
    "ya = (y-ym)/ystd\n",
    "#\n",
    "np.savez(dirout+f'ML_params_v{ver}.npz',Xm=Xm,Xstd=Xstd,ym=ym,ystd=ystd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2cd24-e662-4fd4-86db-0d1faefd32ab",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa272135-40d3-45e1-91d2-594a93fdec5c",
   "metadata": {},
   "source": [
    "### Manually configure K-fold cross validation\n",
    "- 80-20 split by randomly selecting 11 years\n",
    "- K-fold CV with split by decade\n",
    "- Skip the next 3 cells if re-using the previous train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba4df81-75e2-4d37-99d2-0c33f44d6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1971 1993 1967 1992 1989 1981 1996 1998 1994 1990 2001]\n"
     ]
    }
   ],
   "source": [
    "# determine which year to be used for test data\n",
    "if rerun==False:\n",
    "    yr_drop = np.random.choice(yrs,11,replace=False)\n",
    "    print(yr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2110e184-b089-461a-a75e-ac57bcb71a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the count of data point (bins) = 2120442\n",
      "the count of test data (bins) = 429558, which is 20.25794622064645%\n"
     ]
    }
   ],
   "source": [
    "# group these years together into a single array\n",
    "if rerun==False:\n",
    "    yr1=np.round(tt1/12+1965)\n",
    "    ind=(yr1==int(yr_drop[0]))\n",
    "    N=np.sum(ind)\n",
    "    for n in np.arange(1,11,1):\n",
    "        tmp=(yr1==yr_drop[n])\n",
    "        ind=(ind==True)|(tmp==True)\n",
    "        N=N+np.sum(tmp)\n",
    "    #print(N,np.sum(ind))\n",
    "    print(f'the count of data point (bins) = {yr1.size}')\n",
    "    print(f'the count of test data (bins) = {N}, which is {N/yr1.size*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e2a420-9a43-4e0d-b210-fe7e01ceab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the count of train data point (bins) = 1690884\n"
     ]
    }
   ],
   "source": [
    "# Assemble into input data (train/test) and save it for record\n",
    "if rerun==False:\n",
    "    ind1 = (ind==False)\n",
    "    X_train = Xa[ind1,:]\n",
    "    X_test = Xa[ind,:]\n",
    "    y_train = ya[ind1]\n",
    "    y_test = ya[ind]\n",
    "    print(f'the count of train data point (bins) = {y_train.size}')\n",
    "    np.savez(dirout+f'train_test_v{ver}_{date1}.npz',X_train=X_train,X_test=X_test,\n",
    "             y_train=y_train,y_test=y_test,yr_drop=yr_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e660b32-a42f-4df6-aae4-3d3cd9254949",
   "metadata": {},
   "source": [
    "### Start here to re-use previous train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ced9d56-d08f-4e30-ab1c-bded30404a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the saved input data file\n",
    "tmp=np.load(dirout+f'train_test_v{ver}_{date2}.npz')\n",
    "X_train = tmp['X_train']\n",
    "X_test  = tmp['X_test']\n",
    "y_train = tmp['y_train']\n",
    "y_test  = tmp['y_test']\n",
    "#\n",
    "tmp = np.load(dirout+f'ML_params_v{ver}.npz')\n",
    "Xstd = tmp['Xstd']\n",
    "Xm   = tmp['Xm']\n",
    "#\n",
    "ttmp0 = X_train[:,5]*Xstd[5]+Xm[5]\n",
    "yr1 = ttmp0/12+1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c0d341-f69f-468a-a70f-fc7da07ab300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total count of data points = 1690884\n",
      "N,train = 1690884, Group 0 train size = 1258309, Group 0 test size = 432575, 25.582772088446042%\n",
      "N,train = 1690884, Group 1 train size = 1118137, Group 1 test size = 572747, 33.87263703482912%\n",
      "N,train = 1690884, Group 2 train size = 1391724, Group 2 test size = 299160, 17.692520598692756%\n",
      "N,train = 1690884, Group 3 train size = 1547416, Group 3 test size = 143468, 8.484792570040286%\n",
      "N,train = 1690884, Group 4 train size = 1451421, Group 4 test size = 239463, 14.162000468394048%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Decadal Group K-fold\n",
    "tbnds=[1965,1975,1985,1995,2005,2020]\n",
    "Kval = 5\n",
    "#yr1=X[5,ind1]/12+1965\n",
    "print(f'The total count of data points = {yr1.size}')\n",
    "for n in range(5):\n",
    "    K_test=(yr1>=tbnds[n])&(yr1<tbnds[n+1])\n",
    "    K_train=(K_test==False)\n",
    "    X_trainK = X_train[K_train,:]\n",
    "    X_testK = X_train[K_test,:]\n",
    "    y_trainK = y_train[K_train]\n",
    "    y_testK = y_train[K_test]\n",
    "    # check\n",
    "    print(f'N,train = {y_train.size}, Group {n} train size = {y_trainK.size}, Group {n} test size = {y_testK.size}, {y_testK.size/y_train.size*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad2420-67b3-4b22-bdac-3dfe7cedcdaa",
   "metadata": {},
   "source": [
    "### Algorithm selection & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d85fa8f-2bdd-4d6b-80cc-e036b4819c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_parameters = {'n_estimators':[50,100,200,400,600,800],'min_samples_split':[2,3,5]}\n",
    "NN_parameters = {'hidden_layer_sizes':[[10,10,10,10],[20,20,20,20],[40,40,40,40],\n",
    "                                       [60,60,60,60],[60,40,20,10],[20,20,20,20,20,20,10,5]],'alpha':[.001, .01, .1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fca52a7-df7a-4580-a711-acd90086c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_K(k):\n",
    "    if alg =='RF':\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        msp=RF_parameters['min_samples_split'][parm2]\n",
    "        nest=RF_parameters['n_estimators'][parm1]\n",
    "        msl=5\n",
    "        regr=RandomForestRegressor(n_jobs=-1,n_estimators=nest,min_samples_split=msp,\n",
    "                                   min_samples_leaf=msl,max_features='sqrt')\n",
    "        K_test=(yr1>=tbnds[k])&(yr1<tbnds[k+1])\n",
    "        K_train=(K_test==False)\n",
    "        X_trainK = X_train[K_train,:]\n",
    "        X_testK = X_train[K_test,:]\n",
    "        y_trainK = y_train[K_train]\n",
    "        y_testK = y_train[K_test]\n",
    "        regr.fit(X_trainK, y_trainK)\n",
    "        y_est = regr.predict(X_testK)\n",
    "        np.savez(dirout+f'RFtest_pred_v{ver}_cv{k}_{parm1}_{parm2}.npz',Xtest=X_testK,test=y_testK,est=y_est)\n",
    "    elif alg == 'NN':\n",
    "        from sklearn.neural_network import MLPRegressor\n",
    "        hls=NN_parameters['hidden_layer_sizes'][parm1]\n",
    "        alp=NN_parameters['alpha'][parm2]\n",
    "        regr=MLPRegressor(max_iter=1000,hidden_layer_sizes=hls,alpha=alp)\n",
    "        K_test=(yr1>=tbnds[k])&(yr1<tbnds[k+1])\n",
    "        K_train=(K_test==False)\n",
    "        X_trainK = X_train[K_train,:]\n",
    "        X_testK = X_train[K_test,:]\n",
    "        y_trainK = y_train[K_train]\n",
    "        y_testK = y_train[K_test]\n",
    "        regr.fit(X_trainK, y_trainK)\n",
    "        y_est = regr.predict(X_testK)\n",
    "        np.savez(dirout+f'NNtest_pred_v{ver}_cv{k}_{parm1}_{parm2}.npz',Xtest=X_testK,test=y_testK,est=y_est)\n",
    "    r=np.corrcoef(y_est,y_testK)\n",
    "    return np.round(r[0,1]**2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77541ade-0da2-4b16-8148-570fe873c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9222, 0.9181, 0.8889, 0.8626, 0.8893]\n",
      "[0.9237, 0.9181, 0.8885, 0.8732, 0.8807]\n",
      "[0.9195, 0.913, 0.8825, 0.8639, 0.8842]\n",
      "[0.9307, 0.9236, 0.8959, 0.8724, 0.8931]\n",
      "[0.9299, 0.9244, 0.8952, 0.8696, 0.8794]\n",
      "[0.9263, 0.921, 0.8884, 0.8707, 0.8638]\n",
      "[0.9176, 0.9235, 0.888, 0.8594, 0.5636]\n",
      "[0.915, 0.9229, 0.8874, 0.8603, 0.7546]\n",
      "[0.9256, 0.9217, 0.8954, 0.8734, 0.8668]\n",
      "[0.8863, 0.9195, 0.8771, 0.8588, 0.7874]\n"
     ]
    }
   ],
   "source": [
    "# save the normalization factors first\n",
    "# then perform gridsearch K-fold cross validation\n",
    "for parm1 in range(6):\n",
    "#for parm1 in [5]:\n",
    "    for parm2 in range(3):\n",
    "    #for parm2 in [1,2]:\n",
    "        if alg =='NN':\n",
    "            from multiprocessing import Pool\n",
    "            if __name__ == '__main__':\n",
    "                with Pool(5) as p:\n",
    "                    print(p.map(train_K, [0, 1, 2, 3, 4]))\n",
    "        elif alg=='RF':\n",
    "            for n in range(5):\n",
    "                r2=train_K(n)\n",
    "                print(n,parm1,parm2,r2)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebae181-bb12-4775-9201-0bb3cfb3c6a3",
   "metadata": {},
   "source": [
    "### Completed. Next, evaluate the results in o2_eval_XXXX script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ca782-b0a7-4618-8e78-a41f70d17730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4O2_v2",
   "language": "python",
   "name": "ml4o2_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
